# Break-down Plots for Additive Variable Attributions {#breakDown}

In Chapter \@ref(ceterisParibusOscillations), we introduced a method for assessment of local variable-importance based on Ceteris-paribus (CP) profiles. The main disadvantage of this method is that the sum of the developed importance scores does not equal the final model prediction.

In this chapter we introduce Break-down (BD) plots, which offer a solution to this problem. Note that the method is similar to the `EXPLAIN` algorithm introduced in  [@explainPaper] and implemented in the `ExplainPrediction` package [@explainPackage]. [TOMASZ: DIFFERENCE IN THE NAME IN THE REFERENCES.]

## Intuition {#BDIntuition}
The underlying idea is to calculate contribution of an explanatory variable to model's prediction as a shift in the expected model response after conditioning on other variables. [TOMASZ: PERHAOS BRING SOME PART OF THE LINEAR MODEL HERE?.]

The idea is illustrated in Figure \@ref(fig:BDPrice4). Consider the prediction for `johny_d` for the random-forest model (see Section \@ref{model-titanic-rf}) for the Titanic data. [TOMASZ: ISSUE - JOHNY_D EMABRKED IN BELFAST, NOT IN SOUTHAMPTON.] Panel A shows distribution of model predictions. The row `all data` shows the distribution of the predictions for the entire dataset. The red dot indicates the average and it is an estimate of the expected model prediction $E_X[f(X)]$ over the distribution of all explanatory variables.

To evaluate the contribution of the explanatory variables to the particular instance prediction, we consider the predictions when fixing the values of the variables. For instance, the row `class=1st` in Panel A of Figure \@ref(fig:BDPrice4) presents the distribution of the predictions obtained when the value of the `class` variable has been fixed to the `1st` class. Again, the red dot indicates the average of the predictions. The next row (`age=8`) shows the distribution and the average predictions with the value of variable `class` set to `1st` and `age` set to `8`, and so on. The last row corresponds to the prediction for `model response for `johny_d`.

The black lines in Panel A show how the inidividual predictions change after the value of the $j$-th variable has been replaced by the value indicated in the name of the row. 

Eventually, however, we may be interested in the average predictions, as indicated in Panel B of Figure \@ref(fig:BDPrice4), or even only in the changes of the averages, as shown in Panel C. In Panel C, positive changes are presented with green bars, while negative differences are marked with red bar. The changes sum up to the final  prediction, which is illustrated by the violet bar at the bottom of Panel C.

[TOMASZ: I LACK SOME SUBSTANTIVE COMMENTS ABOUT THE CONTENTS/INTERPRETATION OF THE PLOTS.]

```{r BDPrice4, echo=FALSE, fig.cap="(fig:BDPrice4) Break-down plots show how the contribution of individual explanatory variables change the average model prediction to the prediction for a single instance (observation). Panel A) The first row shows the distribution and the average (red dot) of model predictions for all data. The next rows show the dirstribution and the average of the predictions when fixing values of subseqeunt explanatory variables. The last row shows the prediction for a particular instance of interest. B) Red dots indicate the average predictions from Panel B. C) The green and red bars indicate, resspectively, positive and negative changes in the average predictions (variable contributions). ", out.width = '80%', fig.align='center'}
knitr::include_graphics("figure/break_down_distr.png")
```

## Method {#BDMethod}

Let $v(j, x_*)$ denote the variable-importance measure of the $j$-th variable and instance $x_*$, i.e., the contribution of the $j$-th variable to prediction at $x_*$. 

We would like the sum of the variable-importance measures for all explanatory variables to be equal to the instance prediction (property called *local accuracy*), so that
$$
f(x_*) = v_0 + \sum_{j=1}^p v(j, x_*),
$$
where $v_0$ denote the average model response. If we re-write that the equation above may be rewritten as follows:
$$
E_X[f(X)|X^1 = x^1_*, \ldots, X^p = x^p_*] = E_X[f(X)] + \sum_{j=1}^p v(j, x_*),
$$
then a natural proposal for $v(j, x_*)$ is

$$
v(j, x_*) = E_X[f(X) | X^1 = x^1_*, \ldots, X^j = x^j_*] - E_X[f(X) | X^1 = x^1_*, \ldots, X^{j-1} = x^{j-1}_*]. 
$$
In other words, the contribution of the $j$-th variable is the difference between the expected value of the prediction conditional on setting the values of the first $j$ variables equal to their values in $x_*$ and the expected value conditional on setting the values of the first $j-1$ variables equal to their values in $x_*$.

To consider more general cases, let $J$ denote a set of $K$ indices from ${1,2,\ldots,p}$. Furthermore, let's define 
$$
\Delta^{j|J} = E_X[f(X) | X^{j_1} = x_*^{j_1},\ldots,X^{j_K} = x_*^{j_K},X^{j} = x_*^{j}] - E_X[f(X) | X^{j_1} = x_*^{j_1},\ldots,X^{j_K} = x_*^{j_K}].
$$
In other words, $\Delta^{j|J}$ is the change between conditional on the expected prediction when setting the values of the explanatory variables with indices from the set $J \cup \{j\}$ equal to their values in $x_*$ and the expected prediction conditional on setting the values of the explanatory variables with indices from the set $J$ equal to their values in $x_*$. In particular, if $J=\emptyset$, then 
$$
\Delta^{j|\emptyset} = E_X[f(X) | X^{j} = x_*^{j}] - E_X[f(X)].
$$
It follows that
$$
v(j, x_*) = \Delta^{j|\{1,  ..., j-1\}}.
$$
Unfortunately, for non-additive models (that include interactions), the value of so-defined variable-importance measure depends on the order, in which one sets the values of the explanatory variables. Figure \@ref(fig:ordering) presents an example. [TOMASZ: LOOKS LIKE A MODEL FOR THE HR DATA. WHICH ONE?]. For the first ordering, the contribution of variable `age` is equal to  0.01, while for the second ordering  the contribution is equal to 0.13. This is due to the lack of additivity of the model. [TOMASZ: WHICH MODEL? IT HASE NEVER BEEN MENTIONED.] 
 
```{r ordering, echo=FALSE, fig.cap="(fig:ordering) An illustration of the dependence of the variable-contribution values Black dots stand for conditional average, red arrows stands for changes between conditional averages.", out.width = '100%', fig.align='center'}
knitr::include_graphics("figure/ordering.png")
```

There are three approaches that can be used to address the issue of the dependence of $v(j, x_*)$ on the order, in which one sets the values of the explanatory variables.

In the first approach, one chooses an ordering according to which the variables with the largest contributions are selected first. In this chapter, we describe a heuristic behind this approach.

In the second approach, one identifies the interactions that cause a difference in variable-importance measure for different orderings and focuses on those interactions. This approach is discussed in Chapter \@ref(iBreakDown).

Finally, one can calculate an average value of the variance-importance measure across all possible orderings. This approach is presented in Chapter \@ref(shapley).

To choose an ordering according to which the variables with the largest contributions are selected first, one can apply a two-step procedure. In the first step, the explanatory variables are ordered. In the second step, the conditioning is applied according to the chosen order of variables.

In the first step, the ordering is chosen based on the decreasing value of the scores equal to $|\Delta^{k|\emptyset}|$. Note that the absolute value is needed, because the variable contributions can be positive or negative. In the second step, the variable-importance measure for the $j$-th variable is calculated as 
$$
v(j, x_*) = \Delta ^{j|J},
$$
where
$$
J = \{k: |\Delta^{k|\emptyset}| < |\Delta^{j|\emptyset}|\},
$$
that is, $J$ is the set of indices of explanatory variables that have scores $|\Delta^{k|\emptyset}|$ smaller than the corresponding score for variable $j$.

The time complexity of theeach of the two steps of the procedure is $O(p)$, where $p$ is the number of explanatory variables.

## Example: Titanic data {#BDExample} 

Let us consider the random-forest model `titanic_rf_v6` (see Section \@ref(model-titanic-rf) and passenger `johny_d` (see Section \@ref(predictions-titanic)) as the instance of interest in the Titanic data. [TOMASZ; JOHNY_D FROM BELFAST IN SECTION \@ref(predictions-titanic).]

The average of model predictions for all passengers is equal to $v_0 = 0.2356585$. Table \@ref(tab:titanicBreakDownDeltas) presents the scores $|\Delta^{j|\emptyset}|$ and the expected values $E[f(X | X^j = x^j_*)]$. Note that $\Delta^{j|\emptyset}=E[f(X) | X^j = x^j_*]-v_0$ and, since for all variables $E[f(X) | X^j = x^j_*]>v_0$, we have got $E[f(X | X^j = x^j_*)]=|\Delta^{j|\emptyset}|+v_0$. 

Table: (\#tab:titanicBreakDownDeltas) Expected values $E[f(X) | X^j = x^j_*]$ and scores $|\Delta^{j|\emptyset}|$ for the random-forest model `titanic_rf_v6` for the Titanic data and `johny_d`. The scores are sorted in the decreasing order.

|variable $j$ | $E[f(X) | X^j = x^j_*]$ | $|\Delta^{j|\emptyset}|$  |
|:--------|---------:|---------:|
|age      | 0.7407795| 0.5051210|
|class    | 0.6561034| 0.4204449|
|fare     | 0.6141968| 0.3785383|
|sibsp    | 0.4786182| 0.2429597|
|parch    | 0.4679240| 0.2322655|
|embarked | 0.4602620| 0.2246035|
|gender   | 0.3459458| 0.1102873|

```{r, echo=FALSE, eval=FALSE}
library("iBreakDown")
avg = mean(predict(explain_rf_v6$model, type = "prob")[,2])

deltas <- iBreakDown:::calculate_1d_changes(explain_rf_v6$model, 
                                  johny_d, 
                                  explain_rf_v6$data[,colnames(johny_d)], 
                                  predict_function = explain_rf_v6$predict_function)
dftmp <- data.frame(variable = names(deltas), E = unlist(deltas) + 0.2356585, delta = unlist(deltas))
library("dplyr")
dftmp %>% arrange(-delta) %>% knitr::kable()

tmp <- break_down(explain_rf_v6, johny_d)
plot(tmp)
as.data.frame(tmp)[,c(1,5,2)] %>% knitr::kable()
```

Based on the ordering defined by the scores $|\Delta^{j|\emptyset}|$ from Table \@ref(tab:titanicBreakDownDeltas), we can compute the variable-importance measures based on the sequential contributions $\Delta^{j|J}$. The computed values are presented in Table \@ref(tab:titanicBreakDownDeltasConseq).

Table: (\#tab:titanicBreakDownDeltasConseq) Variable-importance measures $\Delta^{j|\{1,\ldots,j\}}$ for the random-forest model `titanic_rf_v6` for the Titanic data and `johny_d` computed by using the ordering of variables defined in Table \@ref(tab:titanicBreakDownDeltas). [TOMASZ: I THOUGHT THAT $v_0 = 0.2356585$. WHY INTERCEPT=0.2353095?]


|variable $j$           |  $E[f(X) | X^{\{1,\ldots,j\}} = x^{\{1,\ldots,j\}}_*)]$ |  $\Delta^{j|\{1,\ldots,j\}}$ |
|:----------------------|-----------:|------------:|
|intercept              |   0.2353095|    0.2353095|
|age = 8                |   0.5051210|    0.2698115|
|class = 1st            |   0.5906969|    0.0855759|
|fare = 72              |   0.5443561|   -0.0463407|
|gender = male          |   0.4611518|   -0.0832043|
|embarked = Southampton |   0.4584422|   -0.0027096|
|sibsp = 0              |   0.4523398|   -0.0061024|
|parch = 0              |   0.4220000|   -0.0303398|
|prediction             |   0.4220000|    0.4220000|


Results from Table \@ref(tab:titanicBreakDownDeltasConseq) are presented as a waterfall plot in Figure \@ref(fig:BDjohnyExample).

```{r BDjohnyExample, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(fig:BDjohnyExample) Break-down plot for the `titanic_rf_v6` model and `johny_d` for the Titanic data.", out.width = '99%', fig.align='center'}
library("iBreakDown")
library("randomForest")
library("DALEX")
library("ggplot2")
load("models/explain_rf_v6.rda")
load("models/johny_d.rda")
bd_rf <- break_down(explain_rf_v6,
                 johny_d)
plot(bd_rf) + ggtitle("Break-down plot for `titanic_rf_v6`")
```


## Pros and cons {#BDProsCons}

BD plots offer a model-agnostic approach that can be applied to any predictive model that returns a single number. The approach offers several advantages. The plots are easy to understand. They are compact; results for many variables may be presented in a small space. The approach reduces to an intuitive interpretation for the generalized-linear models. Numerical complexity of the BD algorithm is linear in the number of explanatory variables.

BD plots for non-additive models may be misleading, as they show only the additive contributions. An important issue is the choice of the ordering of the explanatory variables that is used in the calculation of the variable-importance measures. Also, for models with a large number of variables, the BD plot may be complex and include many variables with small contributions to the instance prediction.

## Code snippets for R {#BDR}

In this section, we present key features of the `iBreakDown` R package [@iBreakDownRPackage] which is a part of the  `DrWhy.AI` universe. The  package covers all methods presented in this chapter. It is available on CRAN and GitHub. More details and examples can be found at https://modeloriented.github.io/iBreakDown/.

For illustration purposes, we use the `titanic_rf_v6` random-forest model for the Titanic data developed in Section \@ref(model-titanic-rf). Recall that it is developed to predict the probability of survival from sinking of Titanic. Instance-level explanations are calculated for a single observation: `johny_d` - an 8-year-old passenger that travelled in the 1st class.

`DALEX` explainers for the model and the `jonhy_d` data are retrieved via `archivist` hooks as listed in Section \@ref(ListOfModelsTitanic). 

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library("randomForest")
explain_rf_v6 <- archivist::aread("pbiecek/models/9b971")

library("DALEX")
johny_d <- archivist::aread("pbiecek/models/e3596")
johny_d
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library("randomForest")
library("DALEX")
load("models/explain_rf_v6.rda")
load("models/johny_d.rda")
```

### Basic use of the `break_down()` function

The `iBreakDown::break_down()` function calculates the variable-importance measures for a selected model and the instance of interest. The result of applying the `break_down()` function is a data frame containg the calculated measures. In the simplest call, the function requires only two arguments: the model explainers and the data frame for  the instance of interest. The call below essentailly re-creates the variable-importance values ($\Delta^{j|\{1,\ldots,j\}}$) presented in Table \@ref(tab:titanicBreakDownDeltasConseq).

```{r, warning=FALSE, message=FALSE}
library("iBreakDown")
bd_rf <- break_down(explain_rf_v6, johny_d)
bd_rf
```

Applying the generic `plot()` function to the object resulting from the application of the `break_down()` function creates a BD plot. In this case, it is the plot from Figure \@ref(fig:BDjohnyExample).
. 
```{r, warning=FALSE, message=FALSE}
plot(bd_rf) 
```

### Advanced use of the `break_down()` function

The function `break_down()` allows more arguments. The most commonly used are:

* `x` - a wrapper over a model created with function `DALEX::explain()`, 
* `new_observation` - an observation to be explained is should be a data frame with structure that matches the training data, 
* `order` - a vector of characters (column names) or integers (column indexes) that specify order of explanatory variables that is used for computing the variable-importance measures. If not specified (default), then a one-step heuristic is used to determine the order, 
* `keep_distributions` - a logical value; if `TRUE`, then additional diagnostic information about conditional distributions is stored in the resulting object and can be plotted with the generic `plot()` function.

In what follows we illustrate the use of the arguments.

First, we will specify the ordering of the explanatory variables. Toward this end we can use integer indexes or variable names. The latter option is prerferable in most cases because of transparency. Additionally, to reduce clutter in the plot, we set `max_features = 3` argument in the `plot()` function. 

```{r, warning=FALSE, message=FALSE}
library("iBreakDown")
bd_rf_order <- break_down(explain_rf_v6,
                 johny_d,
                 order = c("class", "age", "gender", "fare", "parch", "sibsp", "embarked"))
plot(bd_rf_order, max_features = 3) 
```

We can use the`keep_distributions = TRUE` argument to enrich the resulting object with additional information about conditional distributions. Subsequently, we can apply the `plot_distributions = TRUE` argument in the `plot()` function to present the distributions as violin plots. Red dots in the plots indicate the average model predictions.  Thin black lines between violin plots correspond to predictions for individual observations. They can be used to  trace how model predictions change after consecutive conditionings.

```{r, warning=FALSE, message=FALSE}
bd_rf_distr <- break_down(explain_rf_v6,
                 johny_d,
                 order = c("class", "age", "gender", "fare", "parch", "sibsp", "embarked"),
                 keep_distributions = TRUE)
plot(bd_rf_distr, plot_distributions = TRUE) 
```
