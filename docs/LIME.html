<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10 Local Interpretable Model-agnostic Explanations (LIME) | Explanatory Model Analysis</title>
  <meta name="description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="10 Local Interpretable Model-agnostic Explanations (LIME) | Explanatory Model Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  <meta name="github-repo" content="pbiecek/ema" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Local Interpretable Model-agnostic Explanations (LIME) | Explanatory Model Analysis" />
  
  <meta name="twitter:description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  

<meta name="author" content="Przemyslaw Biecek and Tomasz Burzykowski" />


<meta name="date" content="2020-03-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="shapley.html"/>
<link rel="next" href="ceterisParibus.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-5650686-15', 'https://pbiecek.github.io/ema/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>
<style>
.figure {
   padding:40px 0px;
}
</style>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><span style="font-size: large">Explanatory Model Analysis</span><br/>Explore, Explain and Examine<br/>Predictive Models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#notes-to-readers"><i class="fa fa-check"></i><b>1.1</b> Notes to readers</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#the-aim-of-the-book"><i class="fa fa-check"></i><b>1.2</b> The aim of the book</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#three-single-laws"><i class="fa fa-check"></i><b>1.3</b> A bit of philosophy: three laws of model explanation</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#bookstructure"><i class="fa fa-check"></i><b>1.4</b> The structure of this book</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#terminology"><i class="fa fa-check"></i><b>1.5</b> Terminology</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#glass-box-models-vs.black-box-models"><i class="fa fa-check"></i><b>1.6</b> Glass-box models vs. black-box models</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#model-agnostic-vs.model-specific-approach"><i class="fa fa-check"></i><b>1.7</b> Model-agnostic vs. model-specific approach</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#what-is-in-this-book-and-what-is-not"><i class="fa fa-check"></i><b>1.8</b> What is in this book and what is not</a></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#thanksto"><i class="fa fa-check"></i><b>1.9</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html"><i class="fa fa-check"></i><b>2</b> Model Development</a><ul>
<li class="chapter" data-level="2.1" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#MDPIntro"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#MDPprocess"><i class="fa fa-check"></i><b>2.2</b> The Process</a></li>
<li class="chapter" data-level="2.3" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#notation"><i class="fa fa-check"></i><b>2.3</b> Notation</a></li>
<li class="chapter" data-level="2.4" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#data-exploration"><i class="fa fa-check"></i><b>2.4</b> Data exploration</a></li>
<li class="chapter" data-level="2.5" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#notationTraining"><i class="fa fa-check"></i><b>2.5</b> Model training</a></li>
<li class="chapter" data-level="2.6" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#model-understanding"><i class="fa fa-check"></i><b>2.6</b> Model understanding</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html"><i class="fa fa-check"></i><b>3</b> Do-it-yourself with R</a><ul>
<li class="chapter" data-level="3.1" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#what-to-install"><i class="fa fa-check"></i><b>3.1</b> What to install?</a></li>
<li class="chapter" data-level="3.2" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#how-to-work-with-dalex"><i class="fa fa-check"></i><b>3.2</b> How to work with <code>DALEX</code>?</a></li>
<li class="chapter" data-level="3.3" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#how-to-work-with-archivist"><i class="fa fa-check"></i><b>3.3</b> How to work with <code>archivist</code>?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="doItYourselfWithPython.html"><a href="doItYourselfWithPython.html"><i class="fa fa-check"></i><b>4</b> Do-it-yourself with Python</a></li>
<li class="chapter" data-level="5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html"><i class="fa fa-check"></i><b>5</b> Data sets and models</a><ul>
<li class="chapter" data-level="5.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#TitanicDataset"><i class="fa fa-check"></i><b>5.1</b> Sinking of the RMS Titanic</a><ul>
<li class="chapter" data-level="5.1.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#exploration-titanic"><i class="fa fa-check"></i><b>5.1.1</b> Data exploration</a></li>
<li class="chapter" data-level="5.1.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-lmr"><i class="fa fa-check"></i><b>5.1.2</b> Logistic regression model</a></li>
<li class="chapter" data-level="5.1.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-rf"><i class="fa fa-check"></i><b>5.1.3</b> Random forest model</a></li>
<li class="chapter" data-level="5.1.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-gbm"><i class="fa fa-check"></i><b>5.1.4</b> Gradient boosting model</a></li>
<li class="chapter" data-level="5.1.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-svm"><i class="fa fa-check"></i><b>5.1.5</b> Support Vector Machine model</a></li>
<li class="chapter" data-level="5.1.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-titanic"><i class="fa fa-check"></i><b>5.1.6</b> Model predictions</a></li>
<li class="chapter" data-level="5.1.7" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersTitanicRCode"><i class="fa fa-check"></i><b>5.1.7</b> Model adapters</a></li>
<li class="chapter" data-level="5.1.8" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ListOfModelsTitanic"><i class="fa fa-check"></i><b>5.1.8</b> List of objects for the <code>titanic</code> example</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ApartmentDataset"><i class="fa fa-check"></i><b>5.2</b> Apartment prices</a><ul>
<li class="chapter" data-level="5.2.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#exploration-apartments"><i class="fa fa-check"></i><b>5.2.1</b> Data exploration</a></li>
<li class="chapter" data-level="5.2.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-lr"><i class="fa fa-check"></i><b>5.2.2</b> Linear regression model</a></li>
<li class="chapter" data-level="5.2.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-rf"><i class="fa fa-check"></i><b>5.2.3</b> Random forest model</a></li>
<li class="chapter" data-level="5.2.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-svm"><i class="fa fa-check"></i><b>5.2.4</b> Support vector model</a></li>
<li class="chapter" data-level="5.2.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictionsApartments"><i class="fa fa-check"></i><b>5.2.5</b> Model predictions</a></li>
<li class="chapter" data-level="5.2.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersApartmentsRCode"><i class="fa fa-check"></i><b>5.2.6</b> Model adapters</a></li>
<li class="chapter" data-level="5.2.7" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ListOfModelsApartments"><i class="fa fa-check"></i><b>5.2.7</b> List of objects for the <code>apartments</code> example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="instance-level.html"><a href="instance-level.html"><i class="fa fa-check"></i>Instance Level</a></li>
<li class="chapter" data-level="6" data-path="InstanceLevelExploration.html"><a href="InstanceLevelExploration.html"><i class="fa fa-check"></i><b>6</b> Introduction to Instance Level Exploration</a></li>
<li class="chapter" data-level="7" data-path="breakDown.html"><a href="breakDown.html"><i class="fa fa-check"></i><b>7</b> Break-down Plots for Additive Attributions</a><ul>
<li class="chapter" data-level="7.1" data-path="breakDown.html"><a href="breakDown.html#BDIntuition"><i class="fa fa-check"></i><b>7.1</b> Intuition</a></li>
<li class="chapter" data-level="7.2" data-path="breakDown.html"><a href="breakDown.html#BDMethod"><i class="fa fa-check"></i><b>7.2</b> Method</a><ul>
<li class="chapter" data-level="7.2.1" data-path="breakDown.html"><a href="breakDown.html#break-down-for-linear-models"><i class="fa fa-check"></i><b>7.2.1</b> Break-down for linear models</a></li>
<li class="chapter" data-level="7.2.2" data-path="breakDown.html"><a href="breakDown.html#break-down-for-a-general-case"><i class="fa fa-check"></i><b>7.2.2</b> Break-down for a general case</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="breakDown.html"><a href="breakDown.html#BDExample"><i class="fa fa-check"></i><b>7.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="7.4" data-path="breakDown.html"><a href="breakDown.html#BDProsCons"><i class="fa fa-check"></i><b>7.4</b> Pros and cons</a></li>
<li class="chapter" data-level="7.5" data-path="breakDown.html"><a href="breakDown.html#BDR"><i class="fa fa-check"></i><b>7.5</b> Code snippets for R</a><ul>
<li class="chapter" data-level="7.5.1" data-path="breakDown.html"><a href="breakDown.html#basic-use-of-the-variable_attribution-function"><i class="fa fa-check"></i><b>7.5.1</b> Basic use of the <code>variable_attribution()</code> function</a></li>
<li class="chapter" data-level="7.5.2" data-path="breakDown.html"><a href="breakDown.html#advanced-use-of-the-variable_attribution-function"><i class="fa fa-check"></i><b>7.5.2</b> Advanced use of the <code>variable_attribution()</code> function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="iBreakDown.html"><a href="iBreakDown.html"><i class="fa fa-check"></i><b>8</b> Break-down Plots for Interactions (iBreak-down Plots)</a><ul>
<li class="chapter" data-level="8.1" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDIntuition"><i class="fa fa-check"></i><b>8.1</b> Intuition</a></li>
<li class="chapter" data-level="8.2" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDMethod"><i class="fa fa-check"></i><b>8.2</b> Method</a></li>
<li class="chapter" data-level="8.3" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDExample"><i class="fa fa-check"></i><b>8.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="8.4" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDProsCons"><i class="fa fa-check"></i><b>8.4</b> Pros and cons</a></li>
<li class="chapter" data-level="8.5" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDRcode"><i class="fa fa-check"></i><b>8.5</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>9</b> Shapley Additive Explanations (SHAP) and Average Variable Attributions</a><ul>
<li class="chapter" data-level="9.1" data-path="shapley.html"><a href="shapley.html#SHAPIntuition"><i class="fa fa-check"></i><b>9.1</b> Intuition</a></li>
<li class="chapter" data-level="9.2" data-path="shapley.html"><a href="shapley.html#SHAPMethod"><i class="fa fa-check"></i><b>9.2</b> Method</a></li>
<li class="chapter" data-level="9.3" data-path="shapley.html"><a href="shapley.html#SHAPExample"><i class="fa fa-check"></i><b>9.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="9.4" data-path="shapley.html"><a href="shapley.html#SHAProsCons"><i class="fa fa-check"></i><b>9.4</b> Pros and cons</a></li>
<li class="chapter" data-level="9.5" data-path="shapley.html"><a href="shapley.html#SHAPRcode"><i class="fa fa-check"></i><b>9.5</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="LIME.html"><a href="LIME.html"><i class="fa fa-check"></i><b>10</b> Local Interpretable Model-agnostic Explanations (LIME)</a><ul>
<li class="chapter" data-level="10.1" data-path="LIME.html"><a href="LIME.html#LIMEIntroduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="LIME.html"><a href="LIME.html#LIMEIntuition"><i class="fa fa-check"></i><b>10.2</b> Intuition</a></li>
<li class="chapter" data-level="10.3" data-path="LIME.html"><a href="LIME.html#LIMEMethod"><i class="fa fa-check"></i><b>10.3</b> Method</a><ul>
<li class="chapter" data-level="10.3.1" data-path="LIME.html"><a href="LIME.html#interpretable-data-representation"><i class="fa fa-check"></i><b>10.3.1</b> Interpretable data representation</a></li>
<li class="chapter" data-level="10.3.2" data-path="LIME.html"><a href="LIME.html#sampling-around-the-instance-of-interest"><i class="fa fa-check"></i><b>10.3.2</b> Sampling around the instance of interest</a></li>
<li class="chapter" data-level="10.3.3" data-path="LIME.html"><a href="LIME.html#developing-the-glass-box-model"><i class="fa fa-check"></i><b>10.3.3</b> Developing the glass-box model</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="LIME.html"><a href="LIME.html#LIMEExample"><i class="fa fa-check"></i><b>10.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="10.5" data-path="LIME.html"><a href="LIME.html#LIMEProsCons"><i class="fa fa-check"></i><b>10.5</b> Pros and cons</a></li>
<li class="chapter" data-level="10.6" data-path="LIME.html"><a href="LIME.html#LIMERcode"><i class="fa fa-check"></i><b>10.6</b> Code snippets for R</a><ul>
<li class="chapter" data-level="10.6.1" data-path="LIME.html"><a href="LIME.html#the-lime-package"><i class="fa fa-check"></i><b>10.6.1</b> The lime package</a></li>
<li class="chapter" data-level="10.6.2" data-path="LIME.html"><a href="LIME.html#the-localmodel-package"><i class="fa fa-check"></i><b>10.6.2</b> The localModel package</a></li>
<li class="chapter" data-level="10.6.3" data-path="LIME.html"><a href="LIME.html#the-iml-package"><i class="fa fa-check"></i><b>10.6.3</b> The iml package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ceterisParibus.html"><a href="ceterisParibus.html"><i class="fa fa-check"></i><b>11</b> Ceteris-paribus Profiles</a><ul>
<li class="chapter" data-level="11.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPIntro"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPIntuition"><i class="fa fa-check"></i><b>11.2</b> Intuition</a></li>
<li class="chapter" data-level="11.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPMethod"><i class="fa fa-check"></i><b>11.3</b> Method</a></li>
<li class="chapter" data-level="11.4" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPExample"><i class="fa fa-check"></i><b>11.4</b> Example: Titanic</a></li>
<li class="chapter" data-level="11.5" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPProsCons"><i class="fa fa-check"></i><b>11.5</b> Pros and cons</a></li>
<li class="chapter" data-level="11.6" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPR"><i class="fa fa-check"></i><b>11.6</b> Code snippets for R</a><ul>
<li class="chapter" data-level="11.6.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#basic-use-of-the-individual_profile-function"><i class="fa fa-check"></i><b>11.6.1</b> Basic use of the <code>individual_profile</code> function</a></li>
<li class="chapter" data-level="11.6.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#advanced-use-of-the-individual_profile-function"><i class="fa fa-check"></i><b>11.6.2</b> Advanced use of the <code>individual_profile</code> function</a></li>
<li class="chapter" data-level="11.6.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#champion-challenger-analysis"><i class="fa fa-check"></i><b>11.6.3</b> Champion-challenger analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html"><i class="fa fa-check"></i><b>12</b> Ceteris-paribus Oscillations</a><ul>
<li class="chapter" data-level="12.1" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscIntro"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscIntuition"><i class="fa fa-check"></i><b>12.2</b> Intuition</a></li>
<li class="chapter" data-level="12.3" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscMethod"><i class="fa fa-check"></i><b>12.3</b> Method</a></li>
<li class="chapter" data-level="12.4" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscExample"><i class="fa fa-check"></i><b>12.4</b> Example: Titanic</a></li>
<li class="chapter" data-level="12.5" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscProsCons"><i class="fa fa-check"></i><b>12.5</b> Pros and cons</a></li>
<li class="chapter" data-level="12.6" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscR"><i class="fa fa-check"></i><b>12.6</b> Code snippets for R</a><ul>
<li class="chapter" data-level="12.6.1" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#basic-use-of-the-variable_attribution-function-1"><i class="fa fa-check"></i><b>12.6.1</b> Basic use of the <code>variable_attribution</code> function</a></li>
<li class="chapter" data-level="12.6.2" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#advanced-use-of-the-variable_attribution-function-1"><i class="fa fa-check"></i><b>12.6.2</b> Advanced use of the <code>variable_attribution</code> function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="localDiagnostics.html"><a href="localDiagnostics.html"><i class="fa fa-check"></i><b>13</b> Local Diagnostics Plots</a><ul>
<li class="chapter" data-level="13.1" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagIntro"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagIntuition"><i class="fa fa-check"></i><b>13.2</b> Intuition</a></li>
<li class="chapter" data-level="13.3" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagMethod"><i class="fa fa-check"></i><b>13.3</b> Method</a><ul>
<li class="chapter" data-level="13.3.1" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagNeighbors"><i class="fa fa-check"></i><b>13.3.1</b> Nearest neighbors</a></li>
<li class="chapter" data-level="13.3.2" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagLFplot"><i class="fa fa-check"></i><b>13.3.2</b> Local-fidelity plot</a></li>
<li class="chapter" data-level="13.3.3" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagProfiles"><i class="fa fa-check"></i><b>13.3.3</b> Local-stability plot for neighbors</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagExample"><i class="fa fa-check"></i><b>13.4</b> Example: Titanic</a></li>
<li class="chapter" data-level="13.5" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagProsCons"><i class="fa fa-check"></i><b>13.5</b> Pros and cons</a></li>
<li class="chapter" data-level="13.6" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagR"><i class="fa fa-check"></i><b>13.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html"><i class="fa fa-check"></i><b>14</b> Summary of Instance-level Explainers</a><ul>
<li class="chapter" data-level="14.1" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#number-of-explanatory-variables-in-the-model"><i class="fa fa-check"></i><b>14.1</b> Number of explanatory variables in the model</a><ul>
<li class="chapter" data-level="14.1.1" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#low-to-medium-number-of-explanatory-variables"><i class="fa fa-check"></i><b>14.1.1</b> Low to medium number of explanatory variables</a></li>
<li class="chapter" data-level="14.1.2" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#medium-to-large-number-of-explanatory-variables"><i class="fa fa-check"></i><b>14.1.2</b> Medium to large number of explanatory variables</a></li>
<li class="chapter" data-level="14.1.3" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#very-large-number-of-explanatory-variables"><i class="fa fa-check"></i><b>14.1.3</b> Very large number of explanatory variables</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#correlated-explanatory-variables"><i class="fa fa-check"></i><b>14.2</b> Correlated explanatory variables</a></li>
<li class="chapter" data-level="14.3" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#models-with-interactions"><i class="fa fa-check"></i><b>14.3</b> Models with interactions</a></li>
<li class="chapter" data-level="14.4" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#sparse-explanations"><i class="fa fa-check"></i><b>14.4</b> Sparse explanations</a></li>
<li class="chapter" data-level="14.5" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#additional-uses-of-model-exploration-and-explanation"><i class="fa fa-check"></i><b>14.5</b> Additional uses of model exploration and explanation</a></li>
<li class="chapter" data-level="14.6" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#champion-challenger-analysis-1"><i class="fa fa-check"></i><b>14.6</b> Champion Challenger analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="dataset-level.html"><a href="dataset-level.html"><i class="fa fa-check"></i>Dataset Level</a></li>
<li class="chapter" data-level="15" data-path="modelLevelExploration.html"><a href="modelLevelExploration.html"><i class="fa fa-check"></i><b>15</b> Model-level exploration</a></li>
<li class="chapter" data-level="16" data-path="modelPerformance.html"><a href="modelPerformance.html"><i class="fa fa-check"></i><b>16</b> Model Performance Measures</a><ul>
<li class="chapter" data-level="16.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceIntro"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceIntuition"><i class="fa fa-check"></i><b>16.2</b> Intuition</a></li>
<li class="chapter" data-level="16.3" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethod"><i class="fa fa-check"></i><b>16.3</b> Method</a><ul>
<li class="chapter" data-level="16.3.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCont"><i class="fa fa-check"></i><b>16.3.1</b> Continuous dependent variable</a></li>
<li class="chapter" data-level="16.3.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodBin"><i class="fa fa-check"></i><b>16.3.2</b> Binary dependent variable</a></li>
<li class="chapter" data-level="16.3.3" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCateg"><i class="fa fa-check"></i><b>16.3.3</b> Categorical dependent variable</a></li>
<li class="chapter" data-level="16.3.4" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCount"><i class="fa fa-check"></i><b>16.3.4</b> Count dependent variable</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="modelPerformance.html"><a href="modelPerformance.html#example"><i class="fa fa-check"></i><b>16.4</b> Example</a><ul>
<li class="chapter" data-level="16.4.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceApartments"><i class="fa fa-check"></i><b>16.4.1</b> Apartment prices</a></li>
<li class="chapter" data-level="16.4.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceTitanic"><i class="fa fa-check"></i><b>16.4.2</b> Titanic data</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceProsCons"><i class="fa fa-check"></i><b>16.5</b> Pros and cons</a></li>
<li class="chapter" data-level="16.6" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceR"><i class="fa fa-check"></i><b>16.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="featureImportance.html"><a href="featureImportance.html"><i class="fa fa-check"></i><b>17</b> Variable’s Importance</a><ul>
<li class="chapter" data-level="17.1" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceIntro"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceIntuition"><i class="fa fa-check"></i><b>17.2</b> Intuition</a></li>
<li class="chapter" data-level="17.3" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceMethod"><i class="fa fa-check"></i><b>17.3</b> Method</a></li>
<li class="chapter" data-level="17.4" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceTitanic"><i class="fa fa-check"></i><b>17.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="17.5" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceProsCons"><i class="fa fa-check"></i><b>17.5</b> Pros and cons</a></li>
<li class="chapter" data-level="17.6" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceR"><i class="fa fa-check"></i><b>17.6</b> Code snippets for R</a><ul>
<li class="chapter" data-level="17.6.1" data-path="featureImportance.html"><a href="featureImportance.html#models-comparison"><i class="fa fa-check"></i><b>17.6.1</b> Models comparison</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html"><i class="fa fa-check"></i><b>18</b> Partial dependence profiles</a><ul>
<li class="chapter" data-level="18.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPIntro"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPIntuition"><i class="fa fa-check"></i><b>18.2</b> Intuition</a></li>
<li class="chapter" data-level="18.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPMethod"><i class="fa fa-check"></i><b>18.3</b> Method</a><ul>
<li class="chapter" data-level="18.3.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPs"><i class="fa fa-check"></i><b>18.3.1</b> Partial dependence profiles</a></li>
<li class="chapter" data-level="18.3.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clusteredPDPs"><i class="fa fa-check"></i><b>18.3.2</b> Clustered partial dependence profiles</a></li>
<li class="chapter" data-level="18.3.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#groupedPDPs"><i class="fa fa-check"></i><b>18.3.3</b> Grouped partial dependence profiles</a></li>
<li class="chapter" data-level="18.3.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastivePDPs"><i class="fa fa-check"></i><b>18.3.4</b> Contrastive partial dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPExample"><i class="fa fa-check"></i><b>18.4</b> Example: Apartments data</a><ul>
<li class="chapter" data-level="18.4.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#partial-dependence-profiles"><i class="fa fa-check"></i><b>18.4.1</b> Partial dependence profiles</a></li>
<li class="chapter" data-level="18.4.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clustered-partial-dependence-profiles"><i class="fa fa-check"></i><b>18.4.2</b> Clustered partial dependence profiles</a></li>
<li class="chapter" data-level="18.4.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles"><i class="fa fa-check"></i><b>18.4.3</b> Grouped partial dependence profiles</a></li>
<li class="chapter" data-level="18.4.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles"><i class="fa fa-check"></i><b>18.4.4</b> Contrastive partial dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPProsCons"><i class="fa fa-check"></i><b>18.5</b> Pros and cons</a></li>
<li class="chapter" data-level="18.6" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPR"><i class="fa fa-check"></i><b>18.6</b> Code snippets for R</a><ul>
<li class="chapter" data-level="18.6.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clustered-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>18.6.1</b> Clustered partial dependence profiles</a></li>
<li class="chapter" data-level="18.6.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>18.6.2</b> Grouped partial dependence profiles</a></li>
<li class="chapter" data-level="18.6.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>18.6.3</b> Contrastive partial dependence profiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html"><i class="fa fa-check"></i><b>19</b> Local-dependence and Accumulated Local Profiles</a><ul>
<li class="chapter" data-level="19.1" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPIntro"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPIntuition"><i class="fa fa-check"></i><b>19.2</b> Intuition</a></li>
<li class="chapter" data-level="19.3" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPMethod"><i class="fa fa-check"></i><b>19.3</b> Method</a><ul>
<li class="chapter" data-level="19.3.1" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#local-dependence-profile"><i class="fa fa-check"></i><b>19.3.1</b> Local-dependence profile</a></li>
<li class="chapter" data-level="19.3.2" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#accumulated-local-profile"><i class="fa fa-check"></i><b>19.3.2</b> Accumulated local profile</a></li>
<li class="chapter" data-level="19.3.3" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#summaryFeatureEffects"><i class="fa fa-check"></i><b>19.3.3</b> An illustrative example</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#CDPExample"><i class="fa fa-check"></i><b>19.4</b> Example: Apartments data</a></li>
<li class="chapter" data-level="19.5" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPProsCons"><i class="fa fa-check"></i><b>19.5</b> Pros and cons</a></li>
<li class="chapter" data-level="19.6" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPR"><i class="fa fa-check"></i><b>19.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html"><i class="fa fa-check"></i><b>20</b> Residual Diagnostics</a><ul>
<li class="chapter" data-level="20.1" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#IntroResidualDiagnostic"><i class="fa fa-check"></i><b>20.1</b> Introduction</a></li>
<li class="chapter" data-level="20.2" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#IntuitionResidualDiagnostic"><i class="fa fa-check"></i><b>20.2</b> Intuition</a></li>
<li class="chapter" data-level="20.3" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#MethodResidualDiagnostic"><i class="fa fa-check"></i><b>20.3</b> Method</a></li>
<li class="chapter" data-level="20.4" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#ExampleResidualDiagnostic"><i class="fa fa-check"></i><b>20.4</b> Example: Apartments data</a></li>
<li class="chapter" data-level="20.5" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#ProsConsResidualDiagnostic"><i class="fa fa-check"></i><b>20.5</b> Pros and cons</a></li>
<li class="chapter" data-level="20.6" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#RcodeResidualDiagnostic"><i class="fa fa-check"></i><b>20.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="use-cases.html"><a href="use-cases.html"><i class="fa fa-check"></i>Use Cases</a></li>
<li class="chapter" data-level="21" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html"><i class="fa fa-check"></i><b>21</b> FIFA 19</a><ul>
<li class="chapter" data-level="21.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#introduction-1"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#data-preparation"><i class="fa fa-check"></i><b>21.2</b> Data preparation</a></li>
<li class="chapter" data-level="21.3" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#data-understanding"><i class="fa fa-check"></i><b>21.3</b> Data understanding</a></li>
<li class="chapter" data-level="21.4" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#model-assembly"><i class="fa fa-check"></i><b>21.4</b> Model assembly</a></li>
<li class="chapter" data-level="21.5" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#model-audit"><i class="fa fa-check"></i><b>21.5</b> Model audit</a></li>
<li class="chapter" data-level="21.6" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#model-understanding-1"><i class="fa fa-check"></i><b>21.6</b> Model understanding</a></li>
<li class="chapter" data-level="21.7" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#instance-understanding"><i class="fa fa-check"></i><b>21.7</b> Instance understanding</a></li>
<li class="chapter" data-level="21.8" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#cr7"><i class="fa fa-check"></i><b>21.8</b> CR7</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/ModelOriented/DALEX" target="blank">DALEX website</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Explanatory Model Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="LIME" class="section level1">
<h1><span class="header-section-number">10</span> Local Interpretable Model-agnostic Explanations (LIME)</h1>
<div id="LIMEIntroduction" class="section level2">
<h2><span class="header-section-number">10.1</span> Introduction</h2>
<p>Break-down (BD) and Shapley plots, introduced in Chapters <a href="breakDown.html#breakDown">7</a> and <a href="shapley.html#shapley">9</a>, respectively, are most suitable for models with a small or moderate number of explanatory variables.</p>
<p>None of those approaches is well-suited for models with a very large number of explanatory variables. In genomics or image recognition, models with hundreds of thousands or millions of input variables are not uncommon. In such cases, sparse explainers with small number of non zero effects offer a useful alternative. The most popular example of such sparse explainers are Local Interpretable Model-agnostic Explanations (LIME) and their modifications.</p>
<p>The LIME method was originally proposed in ,,Why Should I Trust You?: Explaining the Predictions of Any Classifier’’ <span class="citation">(Ribeiro, Singh, and Guestrin <a href="#ref-lime">2016</a>)</span>. The key idea behind this method is to locally approximate a black-box model by a simpler glass-box model, which is easier to interpret. In this chapter, we describe this approach.</p>
</div>
<div id="LIMEIntuition" class="section level2">
<h2><span class="header-section-number">10.2</span> Intuition</h2>
<p>The intuition behind the LIME method is explained in Figure <a href="LIME.html#fig:limeIntroduction">10.1</a>. We want to understand factors that influence a complex black-box model around a single instance of interest. Areas presented in Figure <a href="LIME.html#fig:limeIntroduction">10.1</a> correspond to decision regions for a binary classifier, i.e., it pertains to a binary dependent variable. The axes represent the values of two continuous explanatory variables. The colored areas correspond to the decision regions, i.e., they indicate for which combinations of the variables the model classifies the observation to one of the two classes. The instance of interest is marked with the large black dot. By using an artificial dataset around the instance of interest, we can use a simpler glass-box model that will locally approximate the predictions of the black-box model. The glass-box model may then serve as a ‘’local explainer’’ for the more complex model.</p>
<p>We may select different classes of glass-box models. The most typical choices are regularized linear models like LASSO regression <span class="citation">(Tibshirani <a href="#ref-Tibshirani94regressionshrinkage">1994</a>)</span> or decision trees <span class="citation">(Hothorn, Hornik, and Zeileis <a href="#ref-party2006">2006</a>)</span>. The important point is to limit the complexity of the models, so that they are easier to explain.</p>

<div class="figure" style="text-align: center"><span id="fig:limeIntroduction"></span>
<img src="figure/lime_introduction.png" alt="The idea behind LIME approximation with local glass-box model. The colored areas correspond to decision regions for a complex binary classification model. The black cross corresponds to the instance of interest x*. Small dots correspond to the generated new data. Size of dots corresponds to proximity to the instance of interest, i.e. to weights w’. Dashed line correspond to a simple linear model fitted for the artificial data. It approximates the black box model around the instance of interest. The simple linear model ,,explains’’ local behaviour of the black box model." width="70%" />
<p class="caption">
Figure 10.1: The idea behind LIME approximation with local glass-box model. The colored areas correspond to decision regions for a complex binary classification model. The black cross corresponds to the instance of interest x*. Small dots correspond to the generated new data. Size of dots corresponds to proximity to the instance of interest, i.e. to weights w’. Dashed line correspond to a simple linear model fitted for the artificial data. It approximates the black box model around the instance of interest. The simple linear model ,,explains’’ local behaviour of the black box model.
</p>
</div>
</div>
<div id="LIMEMethod" class="section level2">
<h2><span class="header-section-number">10.3</span> Method</h2>
<p>As an explanation, we want to find a model that locally approximates a black-box model <span class="math inline">\(f()\)</span> around the instance of interest <span class="math inline">\(x_*\)</span>. Consider class <span class="math inline">\(G\)</span> of interpretable models (linear models or decision trees). To find the required approximation, we consider the following ,,loss function’’</p>
<p><span class="math display">\[
\hat g = \arg \min_{g \in G} L(f, g, \Pi_{x_*}) + \Omega (g), 
\]</span></p>
<p>where model <span class="math inline">\(g()\)</span> belongs to class <span class="math inline">\(G\)</span>, <span class="math inline">\(\Pi_{x_*}\)</span> defines a neighborhood of <span class="math inline">\(x_*\)</span> in which approximation is sought, <span class="math inline">\(L()\)</span> is a fidelity measure between models <span class="math inline">\(f()\)</span> and <span class="math inline">\(g()\)</span>, and <span class="math inline">\(\Omega(g)\)</span> is a penalty for the complexity of model <span class="math inline">\(g()\)</span>. The penalty is used to select simple models from class <span class="math inline">\(G\)</span>.</p>
<p>Note that the models <span class="math inline">\(f()\)</span> and <span class="math inline">\(g()\)</span> may operate on different variable spaces. The black-box model (function) <span class="math inline">\(f(x):\mathcal X \rightarrow \mathcal R\)</span> is defined on the original, large, p-dimensional space <span class="math inline">\(\mathcal X\)</span>. The glass-box model (function) <span class="math inline">\(g:\mathcal X&#39; \rightarrow \mathcal R\)</span> applies to a lower q-dimensional, interpretable space <span class="math inline">\(\mathcal X&#39;\)</span>, and usually <span class="math inline">\(q &lt;&lt; p\)</span>. We will present some examples of <span class="math inline">\(\mathcal X&#39;\)</span> in the next section. For now we will just assume that some function <span class="math inline">\(h()\)</span> transforms <span class="math inline">\(\mathcal X\)</span> into <span class="math inline">\(\mathcal X&#39;\)</span>.</p>
<p>If we limit class <span class="math inline">\(G\)</span> to sparse linear models with <span class="math inline">\(K\)</span> non zero coefficients, the following algorithm may be used to find an interpretable glass-box model <span class="math inline">\(g()\)</span> that includes <span class="math inline">\(K\)</span> most important, interpretable explanatory variables:</p>
<pre><code>Input: x* - observation to be explained
Input: N  - sample size for the glass-box model 
Input: K  - complexity, number of variables for the glass-box model
Input: similarity - distance function in the original input space
1. Let x&#39; = h(x*) be a version of x* in the interpretable space
2. for i in 1...N {
3.   z&#39;[i] &lt;- sample_around(x&#39;) 
     # prediction for a new observation z&#39;[i] 
4.   y&#39;[i] &lt;- f(z[i]) 
5.   w&#39;[i] &lt;- similarity(x&#39;, z&#39;[i]) 
6. }
7. return K-LASSO(y&#39;, x&#39;, w&#39;)</code></pre>
<p>In Step 7, <span class="math inline">\(K-LASSO(y&#39;, x&#39;, w&#39;)\)</span> stands for a weighted LASSO linear-regression that selects <span class="math inline">\(K\)</span> variables based on new dataset <span class="math inline">\((y&#39;, x&#39;)\)</span> with weights <span class="math inline">\(w&#39;\)</span>.</p>
<p>The practical implementation of this idea involves three important steps, which are discussed in the subsequent subsections.</p>
<div id="interpretable-data-representation" class="section level3">
<h3><span class="header-section-number">10.3.1</span> Interpretable data representation</h3>
<p>As it has been mentioned, the black-box model <span class="math inline">\(f()\)</span> and the glass-box model <span class="math inline">\(g()\)</span> operates on different data spaces. For example, let’s consider a VGG16 neural network <span class="citation">(Simonyan and Zisserman <a href="#ref-Simonyan15">2015</a>)</span> trained for ImageNet data <span class="citation">(Deng et al. <a href="#ref-ImageNet">2009</a>)</span>. The model uses an image of the size of <span class="math inline">\(244 \times 244\)</span> pixels as input and predicts to which of 1000 potential categories does the image belong to. The original data space is of dimension <span class="math inline">\(3 \times 244 \times 244\)</span> (three single-color channels <em>red, green, blue</em> for a single pixel <span class="math inline">\(\times 244 \times 244\)</span> pixels), i.e., the input space is 178,608-dimensional. Explaining predictions in such a high-dimensional space is difficult. Instead, the space can be transformed into superpixels, which are treated as binary features that can be turned on or off. Figure <a href="LIME.html#fig:duckHorse06">10.2</a> presents an example of 100 superpixels created for an ambiguous picture. Thus, in this case the black-box model <span class="math inline">\(f()\)</span> operates in principle on data space <span class="math inline">\(\mathcal X=R^{178,608}\)</span>, while the glass-box model <span class="math inline">\(g()\)</span> works on space <span class="math inline">\(\mathcal X&#39; = \{0,1\}^{100}\)</span>.</p>
<p>It is worth noting that superpixels are frequent choices for image data. For text data, words are frequently used as interpretable variables. To reduce to complexity of the data space, continuous variables are often discretized to obtain interpretable tabular data. In case of categorical variables, combination of categories is often used. We will present examples in the next section.</p>

<div class="figure" style="text-align: center"><span id="fig:duckHorse06"></span>
<img src="figure/duck_horse_06.png" alt="The left panel shows an ambiguous picture, half-horse and half-duck. The right panel shows 100 superpixels identified for this figure. Source: https://twitter.com/finmaddison/status/352128550704398338." width="100%" />
<p class="caption">
Figure 10.2: The left panel shows an ambiguous picture, half-horse and half-duck. The right panel shows 100 superpixels identified for this figure. Source: <a href="https://twitter.com/finmaddison/status/352128550704398338" class="uri">https://twitter.com/finmaddison/status/352128550704398338</a>.
</p>
</div>
</div>
<div id="sampling-around-the-instance-of-interest" class="section level3">
<h3><span class="header-section-number">10.3.2</span> Sampling around the instance of interest</h3>
<p>To develop the locally-approximation glass-box model, we need new data points in the interpretable space around the instance of interest. It may not be enough to sample points from the original dataset, because in a high-dimensional data space the data are usually very sparse and data points are ,,far’’ from each other. We need new artificial data points in the interpretable space. For this reason, the data for the development of the glass-box model are often created by using perturbations of the instance of interest.</p>
<p>For a set of binary variables in the interpretable space, the common choice is to flip (from 0 to 1 or from 1 to 0) the value of a randomly-selected number of variables describing the instance of interest.</p>
<p>For continuous variables, various proposals are introduced in different papers. For example ,,iml: An R package for Interpretable Machine Learning’’ <span class="citation">(Molnar, Bischl, and Casalicchio <a href="#ref-imlRPackage">2018</a>)</span> and <span class="citation">(Molnar <a href="#ref-molnar2019">2019</a>)</span> adds some Gaussian noise to continuous variables. In ,,lime: Local Interpretable Model-Agnostic Explanations’’ <span class="citation">(Pedersen and Benesty <a href="#ref-limePackage">2019</a>)</span> continuous variables are discretized with the use of quintiles and the perturbations are don on discretized variables. In ,,localModel: LIME-Based Explanations with Interpretable Inputs Based on Ceteris Paribus Profiles’’ <span class="citation">(Staniak et al. <a href="#ref-localModelPackage">2019</a>)</span> continuous variables are discretized based on segmentation of local Ceteris Paribus profiles.</p>
<p>In the example of the duck-horse in Figure <a href="LIME.html#fig:duckHorse06">10.2</a>, the perturbations of the image would be created by randomly including or excluding some of the superpixels.
See an example in Figure <a href="LIME.html#fig:duckHorseProcess">10.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:duckHorseProcess"></span>
<img src="figure/duck_horse_process.png" alt="In the original input space image is described by RGB colors for each pixel (left panel). The image is transformed into the interpretable input space with 100 super pixels (central panel). The artificial data is a subset of superpixels (right panel)." width="100%" />
<p class="caption">
Figure 10.3: In the original input space image is described by RGB colors for each pixel (left panel). The image is transformed into the interpretable input space with 100 super pixels (central panel). The artificial data is a subset of superpixels (right panel).
</p>
</div>
</div>
<div id="developing-the-glass-box-model" class="section level3">
<h3><span class="header-section-number">10.3.3</span> Developing the glass-box model</h3>
<p>Once the new data were sampled around the instance of interest, we may attempt to develop an interpretable glass-box model <span class="math inline">\(g()\)</span> from class <span class="math inline">\(G\)</span>.</p>
<p>The most common choices for <span class="math inline">\(G\)</span> are generalized linear models. To get sparse models, i.e., models with a limited number of variables, LASSO <span class="citation">(Tibshirani <a href="#ref-Tibshirani94regressionshrinkage">1994</a>)</span> or similar regularization-modelling techniques are used. For instance, in the algorithm presented in Section <a href="LIME.html#LIMEMethod">10.3</a>, the K-LASSO method has been mentioned. An alternative choice are classification-and-regression trees <span class="citation">(Breiman et al. <a href="#ref-CARTtree">1984</a>)</span>.</p>
<p>The VGG16 network for each picture predicts 1000 probabilities that corresponds to the 1000 classes used for training.
For the duck-horse picture the two most likely classes are <em>‘standard poodle’</em> and <em>‘goose’</em>.
Figure <a href="LIME.html#fig:duckHorse04">10.4</a> presents LIME explanations for these top two classes. The explanations were obtained with the K-LASSO method which selected <span class="math inline">\(K\)</span> superpixels that were the most influential from the model-prediction point of view. Here we show results for <span class="math inline">\(K=15\)</span>. For each of the selected two classes, the <span class="math inline">\(K\)</span> superpixels with non-zero coefficients are highlighted. It is interesting to observe that the superpixel which contains the beak is influential for the prediction <em>‘goose’</em>, while the superpixels linked with the white colour are influential for the prediction <em>‘standard poodle’</em>. This is aligned with the intention thus such additional validation increases trust in model prediction.</p>
<div class="figure" style="text-align: center"><span id="fig:duckHorse04"></span>
<img src="figure/duck_horse_04.png" alt="LIME for two predictions ('standard poodle' and 'goose') obtained by the VGG16 network with ImageNet weights for the half-duck, half-horse image." width="100%" />
<p class="caption">
Figure 10.4: LIME for two predictions (‘standard poodle’ and ‘goose’) obtained by the VGG16 network with ImageNet weights for the half-duck, half-horse image.
</p>
</div>
</div>
</div>
<div id="LIMEExample" class="section level2">
<h2><span class="header-section-number">10.4</span> Example: Titanic data</h2>
<p>Most examples of LIME method are related to the text or image data. Here we present examples for tabular data to facilitate comparisons between methods introduced in different chapters.
Let us consider the random-forest model <code>titanic_rf_v6</code> (see Section <a href="dataSetsIntro.html#model-titanic-rf">5.1.3</a>) and passenger <code>johny_d</code> (see Section <a href="dataSetsIntro.html#predictions-titanic">5.1.6</a>) as the instance of interest in the Titanic data.</p>
<p>First, we need to define an interpretable input space. One option would be to gather similar variables into larger constructs corresponding to concepts. For example <code>class</code> and <code>fare</code> variables can be combined into a concept <code>wealth</code>, <code>age</code> and <code>gender</code> into a concept <code>demography</code> and so on. In this example we have relatively small number of variables so we will use a simpler interpretable data representation in the form of a binary vector. Each variable is dychotomized into two levels. For example <code>age</code> is transformed into a binary variable <code>&lt;=</code>/<code>&gt;</code> than 15, <code>class</code> is transformed into a binary variable <code>1st</code>/<code>2nd</code>/<code>deck crew</code> and so on.
The LIME algorithm is applied to this interpretable feature space and the K-LASSO method with <span class="math inline">\(K=3\)</span> is used to identify 3 most important variables that will be transformed into an explanation.</p>
<p>Once the interpretable variable space is defined, we need to transform <code>johny_d</code> to this space and generate a new dataset that will be used for K-LASSO approximations of random forest model. Figure <a href="LIME.html#fig:LIMEexample01">10.5</a> shows coefficients estimated in this K-LASSO model.</p>
<p>The three variables that are identified as the most influential are: <code>age</code>, <code>gender</code>, and <code>class</code>. Note that, for age, a dichotomized version of the originally continuous variable is used. On the other hand, for class, a dichotomized version based on the combination of several original categories is used.</p>

<div class="figure" style="text-align: center"><span id="fig:LIMEexample01"></span>
<img src="figure/LIMEexample01.png" alt="LIME method for the prediction for johny_d for the random-forest model titanic_rf_v6 and the Titanic data. Presented values are beta coefficients in the K-LASSO model fitted locally to the response from the original model." width="60%" />
<p class="caption">
Figure 10.5: LIME method for the prediction for <code>johny_d</code> for the random-forest model <code>titanic_rf_v6</code> and the Titanic data. Presented values are beta coefficients in the K-LASSO model fitted locally to the response from the original model.
</p>
</div>
<p>The interpretable features can be defined in a many different ways. One idea would to be use quartiles for the feature of interest. Another idea is to use Ceteris Paribus profiles (see Chapter <a href="ceterisParibus.html#ceterisParibus">11</a> and change-point method <span class="citation">(Picard <a href="#ref-picard_1985">1985</a>)</span> to find a instance specific discretization.
Different implementations of LIME differ in the way how the interpretable feature space is created.</p>
</div>
<div id="LIMEProsCons" class="section level2">
<h2><span class="header-section-number">10.5</span> Pros and cons</h2>
<p>As mentioned by ,,Why Should I Trust You?: Explaining the Predictions of Any Classifier’’ <span class="citation">(Ribeiro, Singh, and Guestrin <a href="#ref-lime">2016</a>)</span>, the LIME method</p>
<ul>
<li>is <em>model-agnostic</em>, as it does not imply any assumptions on the black-box model structure,</li>
<li>offers an <em>interpretable representation</em>, because the original data space is transformed into a more interpretable lower-dimension space (like transformation from individual pixels to super pixels for image data),</li>
<li>provides <em>local fidelity</em>, i.e., the explanations are locally well-fitted to the black-box model.</li>
</ul>
<p>The method has been widely adopted in text and image analysis, in part due to the interpretable data representation. Also, explanations are delivered as a subset of an image/text and our brain is good in the justification of such explanations. The underlying intuition for the method is easy to understand: a simpler model is used to approximate a more complex one. By using a simpler model, with a smaller number of interpretable explanatory variables, predictions are easier to explain. The LIME method can be applied to complex, high-dimensional models.</p>
<p>But there are several important limitations. For instance, despite several proposals, the issue of finding interpretable representations for continuous and categorical variables is not solved yet. Also, because the glass-box model is selected to approximate the black-box model, and the data themselves, the method does not control the quality of the local fit of the glass-box model to the data. Thus, the latter model may be misleading.</p>
<p>Finally, in high-dimensional data, data points are sparse. Defining a ‘’local neighborhood’’ of the instance of interest may not be straightforward.
Importance of the local neighbourhood is presented for example in the article ,,On the Robustness of Interpretability Methods’’ <span class="citation">(Alvarez-Melis and Jaakkola <a href="#ref-LIMESHAPstability">2018</a>)</span>. Sometimes even slight changes in the neighbourhood affects strongly obtained explanations.</p>
<p>To summarise, the most useful applications of LIME are limited to high dimensional data for which one can defined a low-dimensional interpretable data representation, as in image analysis, text analysis or genomics.</p>
</div>
<div id="LIMERcode" class="section level2">
<h2><span class="header-section-number">10.6</span> Code snippets for R</h2>
<p>LIME and similar methods are implemented in various R and Python packages. For example, <code>lime</code> <span class="citation">(Pedersen and Benesty <a href="#ref-limePackage">2019</a>)</span> is a port of the LIME Python library <span class="citation">(Lundberg <a href="#ref-shapPackage">2019</a>)</span>, while <code>live</code> <span class="citation">(Staniak and Biecek <a href="#ref-R-live">2018</a>)</span>, <code>localModel</code> <span class="citation">(Staniak et al. <a href="#ref-localModelPackage">2019</a>)</span>, and <code>iml</code> <span class="citation">(Molnar, Bischl, and Casalicchio <a href="#ref-imlRPackage">2018</a>)</span> are separate R packages that implements this method from scratch.</p>
<p>Different implementations of LIME offer different algorithms for extraction of interpretable features, different methods for sampling, and different methods of weighting. For instance, regarding transformation of continuous variables into interpretable features, <code>lime</code> performs global discretization using quartiles, <code>localModel</code> performs local discretization using CP profiles, while <code>live</code> and <code>iml</code> work directly on continuous variables.
Due to these differences, the packages yield different results (explanations).</p>
<p>In what follows, for illustration purposes, we use the <code>titanic_rf_v6</code> random-forest model for the Titanic data developed in Section <a href="dataSetsIntro.html#model-titanic-rf">5.1.3</a>. Recall that it is developed to predict the probability of survival from sinking of Titanic. Instance-level explanations are calculated for a single observation: <code>johny_d</code> - an 8-year-old passenger that travelled in the 1st class. <code>DALEX</code> explainers for the model and the <code>jonhy_d</code> data are retrieved via <code>archivist</code> hooks as listed in Section <a href="dataSetsIntro.html#ListOfModelsTitanic">5.1.8</a>.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;DALEX&quot;</span>)</a>
<a class="sourceLine" id="cb73-2" data-line-number="2"><span class="kw">library</span>(<span class="st">&quot;randomForest&quot;</span>)</a>
<a class="sourceLine" id="cb73-3" data-line-number="3"></a>
<a class="sourceLine" id="cb73-4" data-line-number="4">titanic &lt;-<span class="st"> </span>archivist<span class="op">::</span><span class="kw">aread</span>(<span class="st">&quot;pbiecek/models/27e5c&quot;</span>)</a>
<a class="sourceLine" id="cb73-5" data-line-number="5">titanic_rf_v6 &lt;-<span class="st"> </span>archivist<span class="op">::</span><span class="kw">aread</span>(<span class="st">&quot;pbiecek/models/31570&quot;</span>)</a>
<a class="sourceLine" id="cb73-6" data-line-number="6">johny_d &lt;-<span class="st"> </span>archivist<span class="op">::</span><span class="kw">aread</span>(<span class="st">&quot;pbiecek/models/e3596&quot;</span>)</a></code></pre></div>
<div id="the-lime-package" class="section level3">
<h3><span class="header-section-number">10.6.1</span> The lime package</h3>
<p>The key elements of the <code>lime</code> package are functions <code>lime()</code>, which creates an explainer, and <code>explain()</code>, which evaluates explanations.</p>
<p>The detailed results for the <code>titanic_rf_v6</code> random-forest model and <code>johny_d</code> are presented below. First we need to specify that we will work with a model for classification.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;lime&quot;</span>)</a>
<a class="sourceLine" id="cb74-2" data-line-number="2">model_type.randomForest &lt;-<span class="st"> </span><span class="cf">function</span>(x, ...) <span class="st">&quot;classification&quot;</span></a></code></pre></div>
<p>Second we need to create an explainer - an object with all elements needed for calculation of explanations. This can be done with the <code>lime</code> function, the dataset and the model.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" data-line-number="1">lime_rf &lt;-<span class="st"> </span><span class="kw">lime</span>(titanic[,<span class="kw">colnames</span>(johny_d)], titanic_rf_v6)</a></code></pre></div>
<p>In the last step we generate an explanation. The <code>n_features</code> set the K for K-LASSO method. Here we ask for explanations not larger than 4 variables. The <code>n_permutations</code> argument defines how many points are to be sampled for a local model approximation. Here we use a set of 1000 artificial points for this.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" data-line-number="1">lime_expl &lt;-<span class="st"> </span>lime<span class="op">::</span><span class="kw">explain</span>(johny_d, lime_rf, <span class="dt">labels =</span> <span class="st">&quot;yes&quot;</span>, </a>
<a class="sourceLine" id="cb76-2" data-line-number="2">                           <span class="dt">n_features =</span> <span class="dv">4</span>, <span class="dt">n_permutations =</span> <span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb76-3" data-line-number="3">lime_expl</a>
<a class="sourceLine" id="cb76-4" data-line-number="4"></a>
<a class="sourceLine" id="cb76-5" data-line-number="5"><span class="co">#      model_type case label label_prob  model_r2 model_intercept model_prediction</span></a>
<a class="sourceLine" id="cb76-6" data-line-number="6"><span class="co">#1 classification    1    no      0.602 0.5806297       0.5365448        0.5805939</span></a>
<a class="sourceLine" id="cb76-7" data-line-number="7"><span class="co">#2 classification    1    no      0.602 0.5806297       0.5365448        0.5805939</span></a>
<a class="sourceLine" id="cb76-8" data-line-number="8"><span class="co">#3 classification    1    no      0.602 0.5806297       0.5365448        0.5805939</span></a>
<a class="sourceLine" id="cb76-9" data-line-number="9"><span class="co">#4 classification    1    no      0.602 0.5806297       0.5365448        0.5805939</span></a>
<a class="sourceLine" id="cb76-10" data-line-number="10"><span class="co">#  feature feature_value feature_weight  feature_desc                 data   prediction</span></a>
<a class="sourceLine" id="cb76-11" data-line-number="11"><span class="co">#1    fare            72     0.00640936  21.00 &lt; fare 1, 2, 8, 0, 0, 72, 4 0.602, 0.398</span></a>
<a class="sourceLine" id="cb76-12" data-line-number="12"><span class="co">#2  gender             2     0.30481181 gender = male 1, 2, 8, 0, 0, 72, 4 0.602, 0.398</span></a>
<a class="sourceLine" id="cb76-13" data-line-number="13"><span class="co">#3   class             1    -0.16690730   class = 1st 1, 2, 8, 0, 0, 72, 4 0.602, 0.398</span></a>
<a class="sourceLine" id="cb76-14" data-line-number="14"><span class="co">#4     age             8    -0.10026475     age &lt;= 22 1, 2, 8, 0, 0, 72, 4 0.602, 0.398</span></a></code></pre></div>
<p>In this table the <code>feature_weight</code> column has coefficients for the K-LASSO method in the explanation. In the column <code>case</code> one will find an index of observation for which the explanation is calculated. Here it’s 1 since we asked for explanation for only one observation.
The <code>feature_weight</code> columns shows the <span class="math inline">\(\beta\)</span> coefficients in the K-LASSO model, <code>feature</code> column points out which variables have non zero coefficients in the K-LASSO method. The <code>feature_value</code> column denotes values for the selected features for the observation of interest. The <code>feature_description</code> column shows how the original feature was transformed into a interpretable feature.</p>
<p>This implementation of the LIME method dichotomizes continuous variables by using quartiles. Hence, in the output we get a binary variable <code>age &lt;= 22</code>.</p>
<p>The corresponding local white box model is</p>
<p><span class="math display">\[
\hat y = 0.00640936 * 1_{fare &gt; 21} + 0.30481181 * 1_{gender = male} - 
0.16690730 * 1_{class = 1st} -0.10026475 * 1_{age &lt;= 22}
\]</span></p>
<p>Figure <a href="LIME.html#fig:limeExplLIMETitanic">10.6</a> shows the graphical presentation of the results, obtained by applying the generic <code>plot()</code> function.</p>
<p>Color corresponds to the sign of the <span class="math inline">\(\beta\)</span> coefficient while length of the bar corresponds to the absolute value of <span class="math inline">\(\beta\)</span> coefficient in the K-LASSO method.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" data-line-number="1"><span class="kw">plot_features</span>(lime_expl)</a></code></pre></div>

<div class="figure" style="text-align: center"><span id="fig:limeExplLIMETitanic"></span>
<img src="figure/lime_expl_lime_titanic.png" alt="LIME-method results for the prediction for johny_d for the random-forest model titanic_rf_v6 and the Titanic data, generated by the lime package." width="60%" />
<p class="caption">
Figure 10.6: LIME-method results for the prediction for <code>johny_d</code> for the random-forest model <code>titanic_rf_v6</code> and the Titanic data, generated by the <code>lime</code> package.
</p>
</div>
</div>
<div id="the-localmodel-package" class="section level3">
<h3><span class="header-section-number">10.6.2</span> The localModel package</h3>
<p>The <code>localModel</code> package operates on <code>DALEX::explain()</code> object. The main function in this package is <code>individual_surrogate_model()</code> which trains the local glass-box model.</p>
<p>The detailed results for the <code>titanic_rf_v6</code> random-forest model and <code>johny_d</code> are presented below.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;localModel&quot;</span>)</a>
<a class="sourceLine" id="cb78-2" data-line-number="2"></a>
<a class="sourceLine" id="cb78-3" data-line-number="3">explainer_titanic_rf &lt;-<span class="st"> </span>DALEX<span class="op">::</span><span class="kw">explain</span>(<span class="dt">model =</span> titanic_rf_v6,</a>
<a class="sourceLine" id="cb78-4" data-line-number="4">            <span class="dt">data =</span> titanic[,<span class="kw">colnames</span>(johny_d)])</a>
<a class="sourceLine" id="cb78-5" data-line-number="5">local_model_rf &lt;-<span class="st"> </span><span class="kw">individual_surrogate_model</span>(explainer_titanic_rf, </a>
<a class="sourceLine" id="cb78-6" data-line-number="6">            johny_d, <span class="dt">size =</span> <span class="dv">1000</span>, <span class="dt">seed =</span> <span class="dv">1313</span>)</a>
<a class="sourceLine" id="cb78-7" data-line-number="7">local_model_rf</a>
<a class="sourceLine" id="cb78-8" data-line-number="8"><span class="co">#   estimated                    variable dev_ratio response</span></a>
<a class="sourceLine" id="cb78-9" data-line-number="9"><span class="co">#1 0.23479837                (Model mean) 0.6521442         </span></a>
<a class="sourceLine" id="cb78-10" data-line-number="10"><span class="co">#2 0.14483341                 (Intercept) 0.6521442         </span></a>
<a class="sourceLine" id="cb78-11" data-line-number="11"><span class="co">#3 0.08081853 class = 1st, 2nd, deck crew 0.6521442         </span></a>
<a class="sourceLine" id="cb78-12" data-line-number="12"><span class="co">#4 0.00000000     gender = female, NA, NA 0.6521442         </span></a>
<a class="sourceLine" id="cb78-13" data-line-number="13"><span class="co">#5 0.23282293                age &lt;= 15.36 0.6521442         </span></a>
<a class="sourceLine" id="cb78-14" data-line-number="14"><span class="co">#6 0.02338929                fare &gt; 31.05 0.6521442    </span></a></code></pre></div>
<p>In the column <code>estimated</code> one will find <span class="math inline">\(\beta\)</span> coefficients for LASSO logistic regression while in the <code>variable</code> column one will find corresponding values.</p>
<p>The implemented version of LIME dichotomizes continuous variables by using CP profiles. The CP profile for <code>johny_d</code>, presented in Figure <a href="ceterisParibus.html#fig:titanicCeterisProfile01D">11.9</a> in Chapter <a href="ceterisParibus.html#ceterisParibus">11</a>, indicated that, for age, the largest drop in the predicted probability of survival was observed for the age increasing beyond 15 years. Hence, in the output of the <code>individual_surrogate_model()</code>, we see a binary variable <code>age &lt; 15.36</code>.</p>
<p>Figure <a href="LIME.html#fig:LIMEexample02">10.7</a> illustrates how the two levels for age can be extracted from the Ceteris Paribus profile.</p>
<div class="figure" style="text-align: center"><span id="fig:LIMEexample02"></span>
<img src="figure/LIMEexample02.png" alt="Interpretable instance-level discretisation of age variable. Based on the Ceteris Paribus profiles we may estimate an optimal change-point as 15 years." width="60%" />
<p class="caption">
Figure 10.7: Interpretable instance-level discretisation of age variable. Based on the Ceteris Paribus profiles we may estimate an optimal change-point as 15 years.
</p>
</div>
<p>The graphical presentation of the results, obtained by applying the generic <code>plot()</code> function is provided in Figure <a href="LIME.html#fig:limeExplLocalModelTitanic">10.8</a>.
Bars correspond to <span class="math inline">\(\beta\)</span> coefficients in the LASSO model.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" data-line-number="1"><span class="kw">plot</span>(local_model_rf)</a></code></pre></div>

<div class="figure" style="text-align: center"><span id="fig:limeExplLocalModelTitanic"></span>
<img src="figure/lime_expl_localModel_titanic.png" alt="LIME-method results for the prediction for johny_d for the random-forest model titanic_rf_v6 and the Titanic data, generated by the localModel package." width="60%" />
<p class="caption">
Figure 10.8: LIME-method results for the prediction for <code>johny_d</code> for the random-forest model <code>titanic_rf_v6</code> and the Titanic data, generated by the <code>localModel</code> package.
</p>
</div>
</div>
<div id="the-iml-package" class="section level3">
<h3><span class="header-section-number">10.6.3</span> The iml package</h3>
<p>The key elements of the <code>iml</code> package are functions <code>Predictor$new()</code>, which creates an explainer, and <code>LocalModel$new()</code>, which develops the local glass-box model.</p>
<p>The detailed results for the <code>titanic_rf_v6</code> random-forest model and <code>johny_d</code> are presented below.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;iml&quot;</span>)</a>
<a class="sourceLine" id="cb80-2" data-line-number="2">iml_rf =<span class="st"> </span>Predictor<span class="op">$</span><span class="kw">new</span>(titanic_rf_v6, <span class="dt">data =</span> titanic[,<span class="kw">colnames</span>(johny_d)])</a>
<a class="sourceLine" id="cb80-3" data-line-number="3">iml_glass_box =<span class="st"> </span>LocalModel<span class="op">$</span><span class="kw">new</span>(iml_rf, <span class="dt">x.interest =</span> johny_d, <span class="dt">k =</span> <span class="dv">6</span>)</a>
<a class="sourceLine" id="cb80-4" data-line-number="4">iml_glass_box</a>
<a class="sourceLine" id="cb80-5" data-line-number="5"><span class="co">#Interpretation method:  LocalModel </span></a>
<a class="sourceLine" id="cb80-6" data-line-number="6"><span class="co">#</span></a>
<a class="sourceLine" id="cb80-7" data-line-number="7"><span class="co">#Analysed predictor: </span></a>
<a class="sourceLine" id="cb80-8" data-line-number="8"><span class="co">#Prediction task: unknown </span></a>
<a class="sourceLine" id="cb80-9" data-line-number="9"><span class="co">#</span></a>
<a class="sourceLine" id="cb80-10" data-line-number="10"><span class="co">#Analysed data:</span></a>
<a class="sourceLine" id="cb80-11" data-line-number="11"><span class="co">#Sampling from data.frame with 2207 rows and 7 columns.</span></a>
<a class="sourceLine" id="cb80-12" data-line-number="12"><span class="co">#</span></a>
<a class="sourceLine" id="cb80-13" data-line-number="13"><span class="co">#Head of results:</span></a>
<a class="sourceLine" id="cb80-14" data-line-number="14"><span class="co">#          beta x.recoded     effect  x.original              feature</span></a>
<a class="sourceLine" id="cb80-15" data-line-number="15"><span class="co">#1 -0.158368701         1 -0.1583687         1st            class=1st</span></a>
<a class="sourceLine" id="cb80-16" data-line-number="16"><span class="co">#2  1.739826204         1  1.7398262        male          gender=male</span></a>
<a class="sourceLine" id="cb80-17" data-line-number="17"><span class="co">#3  0.018515945         0  0.0000000           0                sibsp</span></a>
<a class="sourceLine" id="cb80-18" data-line-number="18"><span class="co">#4 -0.001484918        72 -0.1069141          72                 fare</span></a>
<a class="sourceLine" id="cb80-19" data-line-number="19"><span class="co">#5  0.131819869         1  0.1318199 Southampton embarked=Southampton</span></a>
<a class="sourceLine" id="cb80-20" data-line-number="20"><span class="co">#6  0.158368701         1  0.1583687         1st            class=1st</span></a></code></pre></div>
<p>In the <code>effect</code> column on can read <span class="math inline">\(\beta\)</span> coefficients for the LASSO method.</p>
<p>The implemented version of LIME does not transform continuous variables. The CP profile for <code>johny_d</code>, presented in Figure <a href="ceterisParibus.html#fig:titanicCeterisProfile01D">11.9</a> in Chapter <a href="ceterisParibus.html#ceterisParibus">11</a>, indicated that, for boys younger than 15-year-old, the predicted probability of survival did not change very much. Hence, in the printed output, age does not appear as an important variable.</p>
<p>The graphical presentation of the results, obtained by applying the generic <code>plot()</code> function to the object resulting from the application of the <code>explain()</code> function, is provided in Figure <a href="LIME.html#fig:limeExplIMLTitanic">10.9</a>. Note that only first 6 rows are listed in the table above. The whole table has 12 coefficients that corresponds to bars in the plot.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" data-line-number="1"><span class="kw">plot</span>(iml_glass_box) </a></code></pre></div>

<div class="figure" style="text-align: center"><span id="fig:limeExplIMLTitanic"></span>
<img src="figure/lime_expl_iml_titanic.png" alt="LIME-method results for the prediction for johny_d for the random-forest model titanic_rf_v6 and the Titanic data, generated by the iml package." width="60%" />
<p class="caption">
Figure 10.9: LIME-method results for the prediction for <code>johny_d</code> for the random-forest model <code>titanic_rf_v6</code> and the Titanic data, generated by the <code>iml</code> package.
</p>
</div>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-LIMESHAPstability">
<p>Alvarez-Melis, David, and Tommi S. Jaakkola. 2018. “On the Robustness of Interpretability Methods.” <em>arXiv E-Prints</em>, June, arXiv:1806.08049.</p>
</div>
<div id="ref-CARTtree">
<p>Breiman, L., J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. <em>Classification and Regression Trees</em>. Monterey, CA: Wadsworth; Brooks.</p>
</div>
<div id="ref-ImageNet">
<p>Deng, J., W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. 2009. “ImageNet: A large-scale hierarchical image database.” In <em>2009 Ieee Conference on Computer Vision and Pattern Recognition</em>, 248–55. <a href="https://doi.org/10.1109/cvpr.2009.5206848">https://doi.org/10.1109/cvpr.2009.5206848</a>.</p>
</div>
<div id="ref-party2006">
<p>Hothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. “Unbiased Recursive Partitioning: A Conditional Inference Framework.” <em>Journal of Computational and Graphical Statistics</em> 15 (3): 651–74.</p>
</div>
<div id="ref-shapPackage">
<p>Lundberg, Scott. 2019. <em>SHAP (SHapley Additive exPlanations)</em>. <a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>.</p>
</div>
<div id="ref-molnar2019">
<p>Molnar, Christoph. 2019. <em>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</em>.</p>
</div>
<div id="ref-imlRPackage">
<p>Molnar, Christoph, Bernd Bischl, and Giuseppe Casalicchio. 2018. “iml: An R package for Interpretable Machine Learning.” <em>Joss</em> 3 (26). Journal of Open Source Software: 786. <a href="https://doi.org/10.21105/joss.00786">https://doi.org/10.21105/joss.00786</a>.</p>
</div>
<div id="ref-limePackage">
<p>Pedersen, Thomas Lin, and Michaël Benesty. 2019. <em>lime: Local Interpretable Model-Agnostic Explanations</em>. <a href="https://CRAN.R-project.org/package=lime">https://CRAN.R-project.org/package=lime</a>.</p>
</div>
<div id="ref-picard_1985">
<p>Picard, Dominique. 1985. “Testing and estimating change-points in time series.” <em>Advances in Applied Probability</em> 17 (4). Cambridge University Press: 841–67. <a href="https://doi.org/10.2307/1427090">https://doi.org/10.2307/1427090</a>.</p>
</div>
<div id="ref-lime">
<p>Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “Why Should I Trust You?: Explaining the Predictions of Any Classifier.” In, 1135–44. ACM Press. <a href="https://doi.org/10.1145/2939672.2939778">https://doi.org/10.1145/2939672.2939778</a>.</p>
</div>
<div id="ref-Simonyan15">
<p>Simonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” In <em>International Conference on Learning Representations</em>.</p>
</div>
<div id="ref-localModelPackage">
<p>Staniak, Mateusz, Przemyslaw Biecek, Krystian Igras, and Alicja Gosiewska. 2019. <em>localModel: LIME-Based Explanations with Interpretable Inputs Based on Ceteris Paribus Profiles</em>. <a href="https://CRAN.R-project.org/package=localModel">https://CRAN.R-project.org/package=localModel</a>.</p>
</div>
<div id="ref-R-live">
<p>Staniak, Mateusz, and Przemysław Biecek. 2018. <em>Live: Local Interpretable (Model-Agnostic) Visual Explanations</em>. <a href="https://CRAN.R-project.org/package=live">https://CRAN.R-project.org/package=live</a>.</p>
</div>
<div id="ref-Tibshirani94regressionshrinkage">
<p>Tibshirani, Robert. 1994. “Regression Shrinkage and Selection Via the Lasso.” <em>Journal of the Royal Statistical Society, Series B</em> 58: 267–88.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="shapley.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ceterisParibus.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ema.pdf", "ema.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
